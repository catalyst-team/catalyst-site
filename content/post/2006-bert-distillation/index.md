---
title: BERT Distillation with Catalyst
date: 2020-06-23
summary: "In the last few years, NLP models have made a big leap in most machine learning tasks. BERT or BERT-based models are the most popular NLP models currently. But there is one big problem - there are about 110 million parameters which could decrease inference speed. Could we get our model smaller without losing quality?"

external_link: https://medium.com/pytorch/catalyst-dev-blog-20-07-release-fb489cd23e14?source=friends_link&sk=7ab92169658fe9a9e1c44068f28cc36c
featured: true
---
