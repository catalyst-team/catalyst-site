---
title: BERT Distillation with Catalyst
date: 2020-06-23
summary: "In the last few years, NLP models have made a big leap in most machine learning tasks. BERT or BERT-based models are the most popular NLP models currently. But there is one big problem - there are about 110 million parameters which could decrease inference speed. Could we get our model smaller without losing quality?"

external_link: https://medium.com/pytorch/bert-distillation-with-catalyst-c6f30c985854?source=friends_link&sk=1a28469ac8c0e6e6ad35bd26dfd95dd9
featured: true
---
